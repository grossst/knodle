{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiener Diarum Sterbelisten\n",
    "-----------------------------------------------------------------------------------------------\n",
    "\n",
    "### Data\n",
    "\n",
    "The digitalised Wiener Zeitung is partially available as TEI XML as well as an extracted HTML on Github. Starting from 1703, there is representative data.This data set contains only a few newspapers per year starting from 1706.\n",
    "\n",
    "Github Repository: https://github.com/acdh-oeaw/sterbelisten\n",
    "\n",
    "### Task\n",
    "\n",
    "Filter out Toponyms from the Sterbelisten. For this sequence labeling task we need to first tokenize the words and use the rules (patterns and keywords) made by the team (Nina) to fit to our classes (Toponym or not).\n",
    "\n",
    "\n",
    "##### What we have \n",
    "There is a list with sucessfully extracted toponyms, which are already normalised orthography.\n",
    "There is a list with rules which are combines Regex and other expressions. \n",
    "There is an html file that has \\<mark> on most toponyms but sometimes on persons and other names too.\n",
    "There are defined classes (Toponym/no Toponym).\n",
    "There are .csv files with Toponyms extracted from Wien Geschichte Wiki.\n",
    "\n",
    "##### What we want\n",
    "A trainig and test set containing three matrixes:\n",
    "T Matrix containing the classes and the rules.(np.matrix, sparse)\n",
    "Z Matrixes each containing the samples and the rules.(np.matrix, sparse)\n",
    "X Matrix containing the samples (data frame)\n",
    "\n",
    "\n",
    "##### What to do\n",
    "\n",
    "**1. Read in the html file**\n",
    "Strip of tags and clean the text with re\n",
    "Expand abbreviations- if possible?\n",
    "create a list with each sentence as an item\n",
    "\n",
    "**2. Read in already extracted placenames**\n",
    "Extract all \\<mark> words from HTML**\n",
    "Using their code\n",
    "clean the placenames\n",
    "create a set from exsiting placenames\n",
    "\n",
    "**3. Building training data**\n",
    "As there is no clean manual training data, it has to be build:\n",
    "\n",
    "**X Matrx**\n",
    "pandas df or list with all sentenses\n",
    "\n",
    "**T Matrix**\n",
    "\n",
    "np.sparse matirx 1 colum 0, 1 colum 1, rows are keywords\n",
    "\n",
    "**Z Matrices**\n",
    "numpy.narray\n",
    "colums= keywords \n",
    "rows = words in sentense\n",
    "\n",
    "\n",
    "\n",
    "**5. Building test data** \n",
    "\n",
    "manually checked! \n",
    "There is the 'timemachine_evaluatuin_v1_edited_corrected.jsonl which should contain manually corrected place names but I don't understand where the actual word is tagged or how to filter out that information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read in HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Load html file'''\n",
    "\n",
    "file = codecs.open('annotations_3-21_v4.html', \"r\", \"utf-8\")\n",
    "sterbelisten_html = file.read()\n",
    "\n",
    "'''Cleaning the text first from both html taggs and xml tags and than in a second step clean the intro and replace with /n'''\n",
    "\n",
    "sterbelisten_strip_html_1 = re.sub(r\"<h2>.*</h2><h3>.*xml \\|.*\\d+</h3><br/>\",\"\",sterbelisten_html)\n",
    "\n",
    "sterbelisten_strip_html1 = re.sub(r\"<hr/>|<p>|</p>|#+|=| =|<html>|</html>|\\r|\\b \\.|\\b# \\.|(|)\",\"\",sterbelisten_strip_html_1)#<mark>|</mark>\n",
    "\n",
    "sterbelisten_strip_html2 = re.sub(r\"    \\b\",\"\",sterbelisten_strip_html1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''create an empty list and split the text at line break, creating von element for each sentense'''\n",
    "\n",
    "sterbeliste = []\n",
    "sterbeliste = sterbelisten_strip_html2.split(\"\\n\\n\\n\")\n",
    "#print(sterbeliste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Create labeled sentenses and dictionary of Place Names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open questions:\n",
    "\n",
    "How to handle Toponyms with more than 2 words - most of them are not really Toponyms but sentenses, where the closing <mark> tag is missing in the html file. \n",
    "    \n",
    "One solution could be exluding the 170 items from the toponym_set, or check them manually. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the sentence: 13\n",
      "Sample_tagging:[None, None, None, None, None, None, 0, 0, None, 1, 1, None, None, None, None, None, None, None]\n",
      "Number of place names: 6\n",
      "=======================================\n",
      "Sentense and labeles: [('Christina', None), ('Kochin', None), ('ein', None), ('Wittib', None), ('im', None), ('Barbieris', 4), ('Hauß', 4), ('auf', None), ('der', None), ('Laimgrueben', 5), ('alt', None), ('67', None), ('Jahr', None)]\n",
      "Cleaned Sentense: ['\\nDem', 'Peter', 'Frost', 'einem', 'Cammer', 'im', 'Greiseckeris', 'Hauß', 'im', 'Diener', 'Gäßl', 'sein', 'Kind', 'Frantz', 'alt', '6', 'viertl', 'Jahr']\n"
     ]
    }
   ],
   "source": [
    "def get_location_id(location: List, locations: List) -> int:\n",
    "    for word in location:\n",
    "        if word not in locations:\n",
    "            locations[word]= len(locations)\n",
    "            return locations[word]\n",
    "\n",
    "locations={}\n",
    "samples = []\n",
    "matched_locations=[]\n",
    "clean_sterbeliste=[]\n",
    "for sentense in sterbeliste[:3]:\n",
    "    labels=[]\n",
    "    sentense = re.sub(r\"<mark>\",\"<start> \",sentense)\n",
    "    sentense = re.sub(r\"</mark>\",\" <end>\",sentense)\n",
    "    sentense = re.sub(r\"\\(|\\)|/\",\"\",sentense)\n",
    "    sentense = sentense.split(\" \")\n",
    "    sentense = list(filter(None,sentense))\n",
    "    if_loc= False\n",
    "    #print(f\"Sentence: {sentense}\")\n",
    "    for word in sentense:\n",
    "        if word=='<start>':\n",
    "            if_loc= True\n",
    "            location = []\n",
    "        elif word=='<end>':\n",
    "            labels +=[get_location_id(location, locations)] * len(location)\n",
    "            if_loc= False\n",
    "        else:\n",
    "            if if_loc:\n",
    "                location.append(word)\n",
    "                #if location not int matched_locations:\n",
    "                #    matched_locations.append(location)\n",
    "            else:\n",
    "                labels.append(None)\n",
    "    samples.append(labels)\n",
    "    sentense= [word for word in sentense if word !=\"<start>\" if word!=\"<end>\"]\n",
    "    clean_sterbeliste.append(sentense)\n",
    "\n",
    "print(f\"Length of the sentence: {len(sentense)}\")\n",
    "print(f\"Sample_tagging:{samples[0]}\")\n",
    "print(f\"Number of place names: {len(locations)}\")\n",
    "print(f\"=======================================\")\n",
    "assert len(sentense) == len(labels)\n",
    "print(f\"Sentense and labeles: {[(word, label) for (word, label) in zip(sentense, labels)]}\")\n",
    "print(f\"Cleaned Sentense: {clean_sterbeliste[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.Building training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "X Matrix\n",
    "Dimensions sentesesxsentenses (one column sample df, len(sample))\n",
    "\n",
    "'''\n",
    "X = pd.DataFrame(clean_sterbeliste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**T Matrix**\n",
    "\n",
    "np.sparse matirx 1 colum 0, 1 colum 1, rows are keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 2)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "T Matrix\n",
    "Dimensions keywords x 2\n",
    "\n",
    "'''\n",
    "import numpy as np\n",
    "a= np.zeros(len(locations))\n",
    "b= np.full(len(locations),1)\n",
    "\n",
    "T = np.stack((b,a.T))\n",
    "T.T.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Z Matrices**\n",
    "Each sentense is going to be its own Z Matrix.\n",
    "colums = place names\n",
    "rows = words in sentense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here I am building the Z Matrixes by looping through all samples.\n",
    "Each sample is compared with the list of locations, if the location is in the sample,\n",
    "the new list \"matched_loc\" gets an entry 1 otherwise a 0 is added\n",
    "Then the list is transformed to an numpy array and brought into shape:\n",
    "number of place names x number of words\n",
    "\n",
    "'''\n",
    "locations_list = list(range(0,(len(locations))))\n",
    "\n",
    "\n",
    "for sample in samples:\n",
    "    matched_loc= [1 if i == j else 0 for i in sample for j in locations_list]\n",
    "    i=len(sample)\n",
    "    Z = np.array(matched_loc)\n",
    "    Z = np.reshape(Z, (i,len(locations_list)))\n",
    "# I haven't found a way to save them with their index number! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'create a tokenized sentenses with spacy'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"create a tokenized sentenses with spacy\"\"\"\n",
    "#import spacy \n",
    "#from spacy.lang.de.examples import sentences\n",
    "\n",
    "#nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "#for i in sterbeliste:\n",
    "    #doc = nlp(i)\n",
    "    #print(doc.text)\n",
    "    #for token in doc:\n",
    "        #print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loading manually checked test data'''\n",
    "\n",
    "with jsl.open('timemachine_evaluation_v1_edited_corrected.jsonl','r') as reader: #read in jsonl with jsonlines\n",
    "    df = pd.json_normalize(reader) \n",
    "\n",
    "### I don't understand where the tagged toponym is or whether there is any"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
