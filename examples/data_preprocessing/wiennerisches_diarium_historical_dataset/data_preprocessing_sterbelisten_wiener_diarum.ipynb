{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiener Diarum Sterbelisten: Data Preprocessing for Sequence Labeling\n",
    "-----------------------------------------------------------------------------------------------\n",
    "\n",
    "### Data\n",
    "\n",
    "The Wiener Zeitung is the oldest running newspaper in the world. It started in 1703 as Wiener Diarum and changed in 1780 to Wiener Zeitung. Through its central role in the Habsburg monarchy it distributed knowledge across all disciplines thorughout Europe. This also helped to put Vienna into a more prominent position.\n",
    "\n",
    "\n",
    "The Austrian Centre for Digital Humanities and Cultural Heritage of the Austrian Academy of Sciences (ACDH-CH) started a project with the goal of digitalising and providing this historically important data. The digitsed newspapers as well as current developements can be found here: \n",
    "https://digitarium.acdh.oeaw.ac.at/willkommen/\n",
    "\n",
    "https://www.oeaw.ac.at/ihb/forschungsbereiche/kunstgeschichte/forschung/habsburgische-repraesentation/das-wiennerische-diarium\n",
    "\n",
    "\n",
    "This valuable resource is useful for multiple disciplines as it holds historical data on science, politics, cultural and other sections. It is interesting for linguistic research as the normalisation of german orthography only started at the end of the 18th cenutry and was only fully set in force in the late 19th cenutry.  \n",
    "\n",
    "For this task there is data prepared by the research group of the ACDH-CH. The digitalised Wiener Zeitung is partially available as TEI XML as well as an extracted HTML on Github. This data set contains only a few newspapers per year starting from 1706.\n",
    "\n",
    "The link to the Github Repository of the ACDH-CH research group shows their current work porcess as well as the original data used for this Tutorial: https://github.com/acdh-oeaw/sterbelisten\n",
    "\n",
    "\n",
    "\n",
    "### Task\n",
    "\n",
    "In this Tutorial we want to prepare the data provided by the research group of the ACDH-CH in order to apply sequence labeling to it and fit the data into the Knodle framework.\n",
    "\n",
    "Our goal is to automatically find the place names in the obituaries. These lists are in every newspaper issue and contain several notes about the death of people. Usually they contain the name, age, reason and place of death. By filtering out the place names automatically, one can extract historical knowledge about the city of Vienna and its developement as well as orthographical changes and therewith historical linguistical knowledge.\n",
    "\n",
    "For this sequence labelling task we need to build the data in a way that is suitable for a weakly supervised machine learning approach. We will need to read the .html file and clean all the additional characters as illustrated later on in this tutorial. We will also tokenise the words given and translate the provided \\<mark> tags into a matrix format that provides the location of the toponym within the sentence. Our rules are the tagged place names, which are a result of the work made by the team of the ACDH-CH. We want to train a model that can detect place names by using the data we are provided with by the research group of the ACDH-CH. As this data is extremly difficult to organise due to the big spelling variation, it is our goal to try a machine learning approach rather than a rule-based approach.\n",
    "\n",
    "\n",
    "##### What we have: provided by the research group of the ACDH-CH\n",
    "The approach of the research group of the ACDH-CH is a rule-based approach on finding place names.  By creating patterns with which they would identify place names and compare them to historical dictionaries, they gather place names in order to link them to other sources. \n",
    "\n",
    "For example regular expressions containing prepositons and other words that would occur in those obituaries (Sterbelisten) are used as patterns. The matched words are then extracted and sent to DTA-CAB (https://www.deutschestextarchiv.de/doku/software); another online tool that compares historically varying spelling by comparing it to historical dictionaries and using sound-distance measures like the Levenshtein distance measure. \n",
    "\n",
    "\n",
    "- A .html file that has \\<mark> on most toponyms but sometimes on persons and other names too.\n",
    "\n",
    "- A .xml file containing the sterbelisten.\n",
    "\n",
    "- A .csv files with Toponyms extracted from Wien Geschichte Wiki.\n",
    "\n",
    "- A .txt file containing a list with sucessfully extracted toponyms, which are already normalised into contemporary orthography by comparing the results with the Wien Geschichte Wiki(https://www.geschichtewiki.wien.gv.at/Wien_Geschichte_Wiki).\n",
    "\n",
    "- Some notebooks and other files.\n",
    "\n",
    "As the ACDH-CH research group is activley working on their project, the Github repository is constantly changing and developing. For our Tutorial we will only use the .html file as it contains the marked place names we need to build our training data.\n",
    "\n",
    "##### What we want: Knodle-compatible input\n",
    "###### A trainig and test set for our sequence labeling task containing three matrices:\n",
    "- T Matrix containing the classes and the rules.(panda data frame with the dimensions 2xnumber of place names).\n",
    "- Z Matrices one for each sentence each containing the words in the sentence and all our rules (=place names)(np.matrix, sparse).\n",
    "- X Matrix containing the samples (pandas data frame).\n",
    "\n",
    "\n",
    "##### Pipeline\n",
    "\n",
    "**1. Read in the html file**\n",
    "\n",
    "- Strip of tags and clean the text with regular expressions.\n",
    "\n",
    "- Create a list with each sentence as a sample.\n",
    "\n",
    "We decided to keep the abbreviations as the expansion would already involve normalisation and we want to try to work with the 'raw' data.\n",
    "\n",
    "\n",
    "**2. Read in already extracted placenames**\n",
    "- Extract all \\<mark> words from the .html file provided by the ACDH-CH research group and assign an ID.\n",
    "- Loop through all samples and create lists containing either None if the word is not a place name or the ID of the place name if the word is part of a place name.\n",
    "\n",
    "\n",
    "**3. Building training data**\n",
    "\n",
    "**X Matrix**\n",
    "pandas data frame or list with all  cleaned sentences.\n",
    "\n",
    "**T Matrix**\n",
    "\n",
    "np.sparse matirx one column 0, one column 1, rows are place names.\n",
    "\n",
    "**Z Matrices**\n",
    "numpy.narray\n",
    "columns= keywords \n",
    "rows = words in sentence\n",
    "\n",
    "\n",
    "**5. Building test data** \n",
    "\n",
    "manually checked! \n",
    "There is the 'timemachine_evaluatuin_v1_edited_corrected.jsonl which should contain manually corrected place names but it is not usable for our purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jsonlines as jsnl\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from joblib import dump\n",
    "\n",
    "from minio import Minio\n",
    "client = Minio(\"knodle.cc\", secure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../../data_from_minio/wiener_diarum_toponyms'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the path to the folder where the data will be stored\n",
    "data_path = \"../../../data_from_minio/wiener_diarum_toponyms\"\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "os.path.join(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "S3Error",
     "evalue": "S3 operation failed; code: NoSuchKey, message: Object does not exist, resource: /knodle/datasets/wiener_diarum_toponyms/a/n/n/o/t/a/t/i/o/n/s/_/3/-/2/1/_/v/4/./h/t/m/l, request_id: 16CE6F7F21CE5D0E, host_id: None, bucket_name: knodle, object_name: datasets/wiener_diarum_toponyms/a/n/n/o/t/a/t/i/o/n/s/_/3/-/2/1/_/v/4/./h/t/m/l",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mS3Error\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12612/2513340338.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#\"datasets/wiener_diarum_toponyms/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     client.fget_object(\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mbucket_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"knodle\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mobject_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"datasets/wiener_diarum_toponyms\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\\\'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\minio\\api.py\u001b[0m in \u001b[0;36mfget_object\u001b[1;34m(self, bucket_name, object_name, file_path, request_headers, ssec, version_id, extra_query_params, tmp_file_path)\u001b[0m\n\u001b[0;32m   1031\u001b[0m         \u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1033\u001b[1;33m         stat = self.stat_object(\n\u001b[0m\u001b[0;32m   1034\u001b[0m             \u001b[0mbucket_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m             \u001b[0mobject_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\minio\\api.py\u001b[0m in \u001b[0;36mstat_object\u001b[1;34m(self, bucket_name, object_name, ssec, version_id, extra_query_params)\u001b[0m\n\u001b[0;32m   1857\u001b[0m         \u001b[0mquery_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextra_query_params\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1858\u001b[0m         \u001b[0mquery_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"versionId\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mversion_id\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mversion_id\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1859\u001b[1;33m         response = self._execute(\n\u001b[0m\u001b[0;32m   1860\u001b[0m             \u001b[1;34m\"HEAD\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1861\u001b[0m             \u001b[0mbucket_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\minio\\api.py\u001b[0m in \u001b[0;36m_execute\u001b[1;34m(self, method, bucket_name, object_name, body, headers, query_params, preload_content, no_body_trace)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m             return self._url_open(\n\u001b[0m\u001b[0;32m    398\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m                 \u001b[0mregion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\minio\\api.py\u001b[0m in \u001b[0;36m_url_open\u001b[1;34m(self, method, region, bucket_name, object_name, body, headers, query_params, preload_content, no_body_trace)\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_region_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbucket_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mresponse_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m     def _execute(\n",
      "\u001b[1;31mS3Error\u001b[0m: S3 operation failed; code: NoSuchKey, message: Object does not exist, resource: /knodle/datasets/wiener_diarum_toponyms/a/n/n/o/t/a/t/i/o/n/s/_/3/-/2/1/_/v/4/./h/t/m/l, request_id: 16CE6F7F21CE5D0E, host_id: None, bucket_name: knodle, object_name: datasets/wiener_diarum_toponyms/a/n/n/o/t/a/t/i/o/n/s/_/3/-/2/1/_/v/4/./h/t/m/l"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "    \"annotations_3-21_v4.html\",\"timemachine_evaluation_v1_edited_corrected.jsonl\"\n",
    "]\n",
    "\n",
    "#\"datasets/wiener_diarum_toponyms/\"\n",
    "for file in tqdm(files):\n",
    "    client.fget_object(\n",
    "        bucket_name=\"knodle\",\n",
    "        object_name=os.path.join(\"datasets/wiener_diarum_toponyms\",*file).replace('\\\\','/'),\n",
    "        file_path=os.path.join(data_path, file[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read in .html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'StreamReaderWriter' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12612/1919067525.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#sterbelisten_html = file.read()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\\\'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0msterbelisten_html\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'StreamReaderWriter' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Load html file\n",
    "\n",
    "#file = codecs.open('annotations_3-21_v4.html', \"r\", \"utf-8\")\n",
    "#sterbelisten_html = file.read()\n",
    "\n",
    "#file= codecs.open(os.path.join(data_path, file[-1]).replace('\\\\','/'), 'r',\"utf-8\" )\n",
    "#sterbelisten_html = file.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .html file contains many tags and information that is redundant and not needed for our nlp task, one example : \n",
    "\n",
    "``` \n",
    "<h2>Lista aller Verstorbenen in und vor der Stadt .  | Den 3 . Februarii 1706 .  | Den 4 . dito .  | Den 5 . Dito . </h2><h3>/db/apps/edoc/data/170x/1706/02/1706-02-03.xml | i51</h3><br/>\\r\\n    <p>Dem Peter# Frost / einem Cammer im <mark>Greiseckeris  Hauß</mark> im <mark>Diener  Gäßl</mark> / sein Kind Frantz / alt 6 . viertl Jahr .####################### </p>\\r\\n<hr/>\\r\\n<h2>Lista aller Verstorbenen in und vor der Stadt .  | Den 3 . Februarii 1706 .  | Den 4 . dito .  | Den 5 . Dito . </h2><h3>/db/apps/edoc/data/170x/1706/02/1706-02-03.xml | i52</h3><br/>\\r\\n    <p>Der Maria# Nauitschanin / einer Burgerl . Wittib im <mark>Primis  Hauß</mark> auf der <mark>Wen=delstadt</mark> / ihr Kind Carl / alt 5 . Jahr .########## </p>\\r\\n<hr/>\\r\\n\n",
    "\n",
    "```\n",
    "We want to clean this document in order to obtain the actual sentence, like here: \n",
    "\n",
    "```\n",
    "\\ Dem Peter Frost / einem Cammer im <mark>Greiseckeris  Hauß</mark> im <mark>Diener  Gäßl</mark> / sein Kind Frantz / alt 6 viertl Jahr\n",
    "```\n",
    "Note that we don't want to lose the \\<mark> and \\<\\mark> tags as we will need them later to collect all place names. The Wiener Diarum also contains a lot of / partially as markers to signal that there is a line break in the sentence or a noun or verb that is seperated into two words. \n",
    "\n",
    "We clean the expression using regex as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the text first from both html tags and xml tags and then in a second step clean the intro and replace with /n\n",
    "\n",
    "# get rid of the title of the issue and the xml tags by just replacing the whole intro with \"\"\n",
    "sterbelisten_strip_html_1 = re.sub(r\"<h2>.*</h2><h3>.*xml \\|.*\\d+</h3><br/>\",\"\",sterbelisten_html)\n",
    "\n",
    "# now working on all the tags around the sentences and weird characters within the words\n",
    "sterbelisten_strip_html1 = re.sub(r\"<hr/>|<p>|</p>|#+|=| =|<html>|</html>|\\r|\\b \\.|\\b# \\.|(|)\",\"\",sterbelisten_strip_html_1)#<mark>|</mark>\n",
    "\n",
    "sterbelisten_strip_html2 = re.sub(r\"    \\b\",\"\",sterbelisten_strip_html1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty list and split the text at line break, creating an element for each sentence\n",
    "\n",
    "sterbeliste = []\n",
    "sterbeliste = sterbelisten_strip_html2.split(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Create labeled sentences and a dictionary of place names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open questions:\n",
    "\n",
    "This code works on the assumption that both \\<mark> and \\<\\mark> tags are present.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of <mark> tags: 14870\n",
      "Number of </mark> tags: 11646\n",
      "Number of unclosed tags: 3224\n",
      "Number of sentenses original data set: 13163\n",
      "Number of sentenses with new, usable <mark> tags in our new data set: 7412\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of <mark> tags: {len(re.findall('<mark>',sterbelisten_html))}\")\n",
    "print(f\"Number of </mark> tags: {len(re.findall('</mark>',sterbelisten_html))}\")\n",
    "print(f\"Number of unclosed tags: {len(re.findall('<mark>',sterbelisten_html))-len(re.findall('</mark>',sterbelisten_html))}\")\n",
    "\n",
    "\n",
    "# We will clean the data set from all sentences were </mark> are missing\n",
    "sterbeliste_clean= [sentence for sentence in sterbeliste if re.search(\"/mark\", sentence)]\n",
    "print(f\"Number of sentenses original data set: {len(sterbeliste)}\")\n",
    "print(f\"Number of sentenses with new, usable <mark> tags in our new data set: {len(sterbeliste_clean)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are 14870 \\<mark> and 11646 \\<\\mark> tags, what can be explained by some errors in preprocessing. \n",
    "We solve this by exluding the 3224 items and their samples from our list, or check them manually and use a cleaned html file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the sentence: 12\n",
      "Sample_tagging:[None, None, None, None, None, None, 0, 0, None, 1, 1, None, None, None, None, None, None, None]\n",
      "Number of place names: 2911\n",
      "=======================================\n",
      "Cleaned Sentence: ['\\nDem', 'Peter', 'Frost', 'einem', 'Cammer', 'im', 'Greiseckeris', 'Hauß', 'im', 'Diener', 'Gäßl', 'sein', 'Kind', 'Frantz', 'alt', '6', 'viertl', 'Jahr']\n"
     ]
    }
   ],
   "source": [
    "def get_location_id(location: List, locations: List) -> int:\n",
    "    '''\n",
    "    this function loops through the list of place names, \n",
    "    if it doesn't find a given place name it adds the place name to the list\n",
    "    and returns the new list \n",
    "    \n",
    "    input: the list or single word that is part of a place name\n",
    "    output: list of place names \n",
    "    '''\n",
    "    for word in location:\n",
    "        if word not in locations:\n",
    "            locations[word]= len(locations)\n",
    "            return locations[word]\n",
    "        else:\n",
    "            return locations[word]\n",
    "\n",
    "# we create a dictionary locations that contains the place name as a key and the ID as a value\n",
    "\n",
    "locations={}\n",
    "\n",
    "# we collect our tagged sentences in samples, these contain None for each word that is no place name and the corresponding ID to each word that is part of a place name \n",
    "samples = []\n",
    "\n",
    "# we collect a list with each sentence that is a list containing each word as its elements\n",
    "clean_sterbeliste=[]\n",
    "\n",
    "\n",
    "# we loop through each sentence\n",
    "for sentence in sterbeliste_clean:\n",
    "    labels=[]\n",
    "    sentence = re.sub(r\"<mark>\",\"<start> \",sentence)\n",
    "    sentence = re.sub(r\"</mark>\",\" <end>\",sentence)\n",
    "    sentence = re.sub(r\"\\(|\\)|/\",\"\",sentence)\n",
    "    sentence = sentence.split(\" \")\n",
    "    sentence = list(filter(None,sentence))\n",
    "    if_loc= False\n",
    "    for word in sentence:\n",
    "        if word=='<start>':\n",
    "            if_loc= True\n",
    "            location = []\n",
    "        elif word=='<end>':\n",
    "            labels +=[get_location_id(location, locations)] * len(location)\n",
    "            if_loc= False\n",
    "        else:\n",
    "            if if_loc:\n",
    "                location.append(word)\n",
    "            else:\n",
    "                labels.append(None)\n",
    "    samples.append(labels)\n",
    "    sentence= [word for word in sentence if word !=\"<start>\" if word!=\"<end>\"]\n",
    "    clean_sterbeliste.append(sentence)\n",
    "\n",
    "print(f\"Length of the sentence: {len(sentence)}\")\n",
    "print(f\"Sample_tagging:{samples[0]}\")\n",
    "print(f\"Number of place names: {len(locations)}\")\n",
    "print(f\"=======================================\")\n",
    "assert len(sentence) == len(labels)\n",
    "#print(f\"Sentence and labeles: {[(word, label) for (word, label) in zip(sentence, labels)]}\")\n",
    "print(f\"Cleaned Sentence: {clean_sterbeliste[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.Building training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the training data, leaving 100 samples as test data\n",
    "\n",
    "d = {\"sample\": clean_sterbeliste, \"labels\":samples}\n",
    "\n",
    "df = pd.DataFrame(d)\n",
    "\n",
    "df_train = df[100:]\n",
    "df_train.to_csv(os.path.join(data_path, 'df_train.csv').replace('\\\\','/'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "X Matrix\n",
    "\n",
    "Dimensions # sentences x # sentences (one column samples df, len(sample))\n",
    "\n",
    "'''\n",
    "x_matrix = pd.DataFrame(clean_sterbeliste)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**T Matrix**\n",
    "\n",
    "np.sparse matirx 1 colum 0, 1 colum 1, rows are keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2911, 2)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "T Matrix\n",
    "Dimensions: # place names x # 2\n",
    "\n",
    "'''\n",
    "a= np.zeros(len(locations))\n",
    "b= np.full(len(locations),1)\n",
    "\n",
    "t_matrix = np.stack((b,a.T))\n",
    "t_matrix.T.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Z Matrices**\n",
    "Each sentense is going to be its own Z Matrix.\n",
    "colums = place names\n",
    "rows = words in sentense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 2911)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Z Matrices\n",
    "\n",
    "In the next function, we are building the Z Matrices by looping through all samples.\n",
    "Each sample is compared with the list of locations, if the location is in the sample,\n",
    "the new list \"matched_loc\" gets an entry 1 otherwise a 0 is added\n",
    "Then the list is transformed to an numpy array and brought into shape:\n",
    "\n",
    "Dimensions\n",
    "number of place names x number of words\n",
    "\n",
    "'''\n",
    "locations_list = list(range(0,(len(locations))))\n",
    "\n",
    "## create a list and turn to np.array 3d \n",
    "\n",
    "collected_z_matrices=[]\n",
    "\n",
    "for sample in samples:\n",
    "    matched_loc= [1 if i == j else 0 for i in sample for j in locations_list]\n",
    "    i=len(sample)\n",
    "    Z = np.array(matched_loc)\n",
    "    Z = np.reshape(Z, (i,len(locations_list)))\n",
    "    collected_z_matrices.append(Z)\n",
    "\n",
    "print(Z.shape)\n",
    "    \n",
    "#padding_length= max([len(i) for i in samples])\n",
    "\n",
    "#z_matrices= np.dstack(collected_z_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../../data_from_minio/wiener_diarum_toponyms/mapping_rules_labels_t.lib']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving the z_matrices\n",
    "\n",
    "'''\n",
    "dump(train_rule_matches_sparse_z, os.path.join(data_path, \"train_rule_matches_z.lib\"))\n",
    "dump(test_rule_matches_sparse_z, os.path.join(data_path, \"test_rule_matches_z.lib\"))\n",
    "\n",
    "'''\n",
    "\n",
    "# saving the t_matix\n",
    "\n",
    "dump(t_matrix,  os.path.join(data_path, \"mapping_rules_labels_t.lib\").replace('\\\\','/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided test data has information about the character position of each place name. \n",
    "The data also considers tags, special characters and other non-verbal characters.\n",
    "For our use, this test data can not be used as we want to train our model to look for place names within a sentence and not on a character level.\n",
    "We might need to take a part of our training data, manually check each sentence and its tagging and use it as our test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building test data, the first 100 samples have been manually checked\n",
    "\n",
    "df_test = df[:100]\n",
    "\n",
    "# Saving them\n",
    "\n",
    "df_test.to_csv(os.path.join(data_path, 'df_test.csv').replace('\\\\','/'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
